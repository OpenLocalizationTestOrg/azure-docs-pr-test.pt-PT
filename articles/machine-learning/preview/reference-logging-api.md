---
title: "Referência da API de registo de ML do Azure | Microsoft Docs"
description: "Referência da API de registo."
services: machine-learning
author: akshaya-a
ms.author: akannava
manager: mwinkle
ms.reviewer: garyericson, jasonwhowell, mldocs
ms.service: machine-learning
ms.workload: data-services
ms.topic: article
ms.date: 09/25/2017
ms.openlocfilehash: 1906425c6657fb6232a9dc306b05f9171c9c7bef
ms.sourcegitcommit: 6699c77dcbd5f8a1a2f21fba3d0a0005ac9ed6b7
ms.translationtype: MT
ms.contentlocale: pt-PT
ms.lasthandoff: 10/11/2017
---
# <a name="logging-api-reference"></a>Referência da API de registo

Biblioteca de registo do Azure ML permite ao programa de emissão métricas e os ficheiros que são controlados pelo serviço de histórico para análise posterior. Atualmente, alguns tipos básicos de métricas e ficheiros são suportados e irá aumentar o conjunto de tipos suportados em versões futuras do pacote do Python.

## <a name="uploading-metrics"></a>Métricas de carregamento

```python
# import logging API package
from azureml.logging import get_azureml_logger

# initialize a logger object
logger = get_azureml_logger()

# log "scalar" metrics
logger.log("simple integer value", 7)
logger.log("simple float value", 3.141592)
logger.log("simple string value", "this is a string metric")

# log a list of numerical values. 
# this automatically creates a chart in the Run History details page
logger.log("chart data points", [1, 3, 5, 10, 6, 4])
```

Por predefinição, todas as métricas são submetidas no modo assíncrono, para que a submissão não impedir a execução do programa. Isto pode causar problemas de ordenação quando várias métricas são enviadas em casos de limite. Um exemplo desta situação seria duas métricas com sessão iniciadas ao mesmo tempo, mas, por algum motivo, que o utilizador prefere ordenação exato ser preservados. Outro cenário é quando a métrica tem de ser controlada antes de executar alguns códigos que é conhecido poderão falhar rápida. Em ambos os casos, a solução é _aguarde_ até que a métrica é totalmente iniciada antes de continuar:

```python
# blocking call
logger.log("my metric 1", 1).wait()
logger.log("my metric 2", 2).wait()
```

## <a name="consuming-metrics"></a>Consumo de métricas

As métricas são armazenadas pelo serviço de histórico e associadas para a execução que produziu-los. O separador de histórico de execuções e o comando da CLI abaixo permitem-lhe obtê-las (e artefactos abaixo) após a execução foi concluída.

```azurecli
# show the last run
$ az ml history last

# list all past runs
$ az ml history list 

# show a paritcular run
$ az ml history info -r <runid>
```

## <a name="artifacts-files"></a>Artefactos (ficheiros)

Para além de métricas, AzureML permite ao utilizador controlar os ficheiros bem. Por predefinição, todos os ficheiros escritos no `outputs` pasta relativos ao diretório de trabalho do programa (a pasta do projeto no contexto de computação) são carregados para o serviço de histórico e controlados por Analysis Services posteriores. A advertência é que o tamanho de ficheiro individual tem de ser inferior a 512 MB.


```Python
# Log content as an artifact
logger.upload("artifact/path", "This should be the contents of artifact/path in the service")
```

## <a name="consuming-artifacts"></a>Consumo de artefactos

Para imprimir o conteúdo de um artefacto que tenha sido controlados, utilizador pode utilizar o separador de histórico de execução para a execução indicada para **transferir** ou **promover** o artefacto ou utilize o abaixo comandos da CLI para conseguir o mesmo efeito.

```azurecli
# show all artifacts generated by a run
$ az ml history info -r <runid> -a <artifact/path>

# promote a particular artifact
$ az ml history promote -r <runid> -ap <artifact/prefix> -n <name of asset to create>
```
## <a name="next-steps"></a>Passos seguintes
- Percorrer o [classificar iris tutoria, parte 2](tutorial-classifying-iris-part-2.md) para ver a API de registo em ação.
- Reveja [como histórico de execuções de utilização e métricas de modelo no Azure Machine Learning Workbench](how-to-use-run-history-model-metrics.md) compreender mais profundo como APIs de registo pode ser utilizado no histórico de execução.
